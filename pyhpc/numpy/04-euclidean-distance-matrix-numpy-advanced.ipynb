{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Euclidean Distance Matrix with NumPy\n",
    "\n",
    "In this notebook we implement two functions to compute the Euclidean distance matrix. We use a simple algebra trick that makes possible to write the function in a completely vectorized way in terms of optimized NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know bradcasting. Let's use it to implement a function that calculates the Euclidean distance matrix of an array of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_broadcast(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = (x_ij - y_ij)^2\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "\n",
    "    return (diff * diff).sum(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Question**</mark>: At this point you are starting to get acquainted with the `numpy.ndarray`s and its memory managment. Could you analyse the advantages and possible drawbacks of the `euclidean_broadcast` function? Write a positive and a negative point about it.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more sophisticated implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_trick(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = (x_ij - y_ij)^2\n",
    "    \"\"\"\n",
    "    x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "    y2 = np.einsum('ij,ij->i', y, y)[np.newaxis, :]\n",
    "\n",
    "    xy = x @ y.T\n",
    "\n",
    "    return np.abs(x2 + y2 - 2. * xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `euclidean_trick` function\n",
    "\n",
    "Each element of the Euclidean distance matrix is the scalar product of the difference between two rows of the array. `euclidean_trick` takes advantage of this by doing the following\n",
    "$$\n",
    "\\sum_k {(x_{ik}-y_{ik})^2} = (\\vec{x}_i - \\vec{y}_j)\\cdot(\\vec{x}_i - \\vec{y}_j) = \\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j - 2\\vec{x}_i\\cdot\\vec{y}_j\n",
    "$$\n",
    "\n",
    "Fortunately, there are NumPy functions to compute each of these terms:\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{y}_j$ $\\rightarrow$ `x @ y.T` : Matrix product of $\\{\\vec{x}\\}$ and $\\{\\vec{y}\\}$\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{x}_i$ $\\rightarrow$ `np.einsum('ij,ij->i', x, x)[:, np.newaxis]` : A $(n,1)$ vector of elements $\\sum_j x_{ij}x_{ij}$\n",
    "\n",
    "$\\vec{y}_j\\cdot\\vec{y}_j$ $\\rightarrow$ `np.einsum('ij,ij->i', y, y)[np.newaxis, :]` : A $(1,n)$ vector of elements $\\sum_j y_{ij}y_{ij}$\n",
    "\n",
    "To have all the combinations $ij$ of the sum $\\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j$, we add a new axis to each of the arrays, transpose one them and add them.\n",
    "\n",
    "Let's see now how the `np.einsum` function works. `einsum` stands for Einstein summation, which is used in tensor algebra to write compact expressions without the sum symbol ($\\sum$). Within the Einstein summation notation, whenever there are repeated indexes, there is a sum over them. For instance, the expression\n",
    "$$x_{ik}y_{kj}$$\n",
    "is equivalent to\n",
    "$$\\sum_k x_{ik}y_{kj}$$\n",
    "\n",
    "`np.einsum` uses a generalized form of the Einstein summation by adding the symbol `->` to prevent summing over certain indexes. The specific operation we use here, `np.einsum('ij,ij->i', x, x)`, gives the vector\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sum_k x_{1k}x_{1k} \\\\\n",
    "\\sum_k x_{2k}x_{2k} \\\\\n",
    " ...                \\\\\n",
    "\\sum_k x_{nk}x_{nk} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that the resulting vector is represented here as a column vector just for visualization purposes. It's is an `(n,)` NumPy array.\n",
    "\n",
    "Let's check now step-by-step what the `euclidean_trick` function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64743471, 0.63074645, 0.53359945],\n",
       "       [0.29225882, 0.41598612, 0.62755519],\n",
       "       [0.53365489, 0.95231284, 0.63856527],\n",
       "       [0.80610192, 0.30077927, 0.47899438],\n",
       "       [0.04441583, 0.47832747, 0.52039788],\n",
       "       [0.01018477, 0.88913597, 0.03319124],\n",
       "       [0.1433456 , 0.13898106, 0.40578775],\n",
       "       [0.69798147, 0.01835295, 0.94325153],\n",
       "       [0.29320895, 0.1975626 , 0.75614765],\n",
       "       [0.29647682, 0.93860601, 0.52665937]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets generate some random data\n",
    "nsamples = 10\n",
    "nfeat = 3\n",
    "\n",
    "# this is the new way of generating random numbers\n",
    "# see https://numpy.org/doc/stable/reference/random/index.html\n",
    "rng = np.random.default_rng()\n",
    "x = rng.random((nsamples, nfeat))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x2 + x2.T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We now use `x @ x.T` to perform the matrix multiplication of the full dataset by itself. We didn't use it before as alternative to `np.einsum` because it doesn't perform row by row scalar products. Instead `x @ x.T`, which calls `np.matmul`, expects two arrays with matching shapes $(m,n)$ and $(n,m)$ to perform a matrix multiplication.\n",
    "\n",
    "We could have used `np.einsum('ik,jk', x, x)` to perform the matrix multiplication, but we chose `x @ x.T` instead. This is because `x @ x.T` is a very sophisticated too, plus it uses OpenMP threads. This results in a very fast execution.\n",
    "\n",
    "You are welcome to time them and look at the `top` command to see how `x @ x.T` uses multiple OpenMP threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = x @ x.T\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, considering that the reason we are using `np.einsum` is to get rid of the loops, why didn't we use something like `(x*x).sum(axis=1)`? Let's run the next cell comparing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 µs ± 634 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "284 µs ± 141 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# let's use a larger array for timing the function calls\n",
    "nsamples = 1000\n",
    "nfeat = 300\n",
    "\n",
    "x = 10. * rng.random((nsamples, nfeat))\n",
    "\n",
    "# it gives the same result\n",
    "np.abs(np.einsum('ij,ij->i', x, x) - (x*x).sum(axis=1)).max()\n",
    "\n",
    "# but it's not as fast as `np.einsum`\n",
    "%timeit np.einsum('ij,ij->i', x, x)\n",
    "%timeit (x * x).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a reduction with the ufunc `np.add` is also slower than `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 µs ± 3.53 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.add.reduce(x * x, axis=1)\n",
    "\n",
    "# As homework, check what's the meaning of the previous line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's time both implementations and check that they give the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64 s ± 5.42 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "54 ms ± 474 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "nsamples = 2000\n",
    "nfeat = 50\n",
    "\n",
    "x = 10. * rng.random((nsamples, nfeat))\n",
    "\n",
    "%timeit euclidean_broadcast(x, x)\n",
    "%timeit euclidean_trick(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that both implementations give the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.637978807091713e-12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(euclidean_broadcast(x, x) - euclidean_trick(x, x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in np.isclose(euclidean_broadcast(x, x), euclidean_trick(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Homework**</mark> Change the implementation of `euclidean_broadcast` function to make faster using `einsum` to do the final sum. How much is the speed-up? Compare it with both the original `euclidean_broadcast` and `euclidean_trick`. Check that the result is the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'solutions/edm-broadcast-einsum.py' was not found in history, as a file, url, nor in the user namespace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/jupyterlab/3.2.8-CrayGNU-21.09-batchspawner-cuda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3569\u001b[0m, in \u001b[0;36mInteractiveShell.find_user_code\u001b[0;34m(self, target, raw, py_only, skip_encoding_cookie, search_ns)\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:                                              \u001b[38;5;66;03m# User namespace\u001b[39;00m\n\u001b[0;32m-> 3569\u001b[0m     codeobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'solutions' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msolutions/edm-broadcast-einsum.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/jupyterlab/3.2.8-CrayGNU-21.09-batchspawner-cuda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2204\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2202\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2204\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/jupyterlab/3.2.8-CrayGNU-21.09-batchspawner-cuda/lib/python3.9/site-packages/IPython/core/magics/code.py:359\u001b[0m, in \u001b[0;36mCodeMagics.load\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m    357\u001b[0m opts,args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_options(arg_s,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myns:r:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m search_ns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m opts\n\u001b[0;32m--> 359\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_user_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_ns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m opts:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/jupyterlab/3.2.8-CrayGNU-21.09-batchspawner-cuda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3571\u001b[0m, in \u001b[0;36mInteractiveShell.find_user_code\u001b[0;34m(self, target, raw, py_only, skip_encoding_cookie, search_ns)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     codeobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns)\n\u001b[1;32m   3570\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was not found in history, as a file, url, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3572\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnor in the user namespace.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m target) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   3574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(codeobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   3575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codeobj\n",
      "\u001b[0;31mValueError\u001b[0m: 'solutions/edm-broadcast-einsum.py' was not found in history, as a file, url, nor in the user namespace."
     ]
    }
   ],
   "source": [
    "%load solutions/edm-broadcast-einsum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The main points to take from this notebook are:\n",
    "  * NumPy is all about vectorization. Loops in python must be avoided.\n",
    "  * Always consider different vectorized implementations and compare them.\n",
    "  * Even within NumPy, some functions might bring a more significant speedup than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
